Topic modeling looks for words that tend

as Andrew Goldstone and Ted Underwood put it in their article on topic modeling *[PMLA](https://andrewgoldstone.com/blog/2012/12/13/pmla/)*, "a 'topic' is neither more nor less than a pattern of co-occurring word"



**Latent Dirichlet Allocation (LDA)**



not dependent on location - just looking at all the different words that show up in a document.



The topic modeling algorithm looks for statistically significant clusters of words. For each document in your corpus, it will look



We call this an **unsupervised classifier** because we are asking the computer to analyze and mark a text without giving it any clear directions. We just say, "here is some text. Do your thing and tell me what you find." A **supervised classifier** would take information from us to help it make decisions. We might say, "read this text. If it has more than fifty uses of the word 'crime' mark it as 'detective fiction.' If it has fifty uses of the word 'sex,' mark it as 'romance.'



Until now, we have stressed approaching text analysis with a clear sense of your interests and the research questions that drive them. Topic modeling works a little differently: it is more useful for exploratory work.







So you've topic modeled! The results areâ€¦confusing. Let's take a look at them.




